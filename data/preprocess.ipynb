{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Char_Embedding_Dim = 30\n",
    "Word_Embedding_Dim = 50\n",
    "Window_Size = 5\n",
    "Number_of_Filter = 100\n",
    "Hidden_Units = 50\n",
    "Dropout_Rate = 0.5\n",
    "Epochs = 50\n",
    "Batch_Size = 10\n",
    "L_rate = 0.001\n",
    "Momentum = 0.9\n",
    "max_num_sentence = 50\n",
    "max_seq_length = 50\n",
    "batch_size =32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Input: training_final.csv file path\n",
    "    output: list of dictionary [{'essay': , 'essay_set':, 'final_score':, 'scaled_score':}, \n",
    "                                {'essay2': , 'essay2_set':, 'final_score2':, 'scaled_score2':}]\n",
    "            @Essay_info_list\n",
    "                \n",
    "    \"\"\"\n",
    "    opt = []\n",
    "    data = pd.read_csv(path, header=0, index_col=0)\n",
    "    data = data.reset_index(drop=True)\n",
    "    data['essay_set'] = [float(n) for n in data['essay_set']]\n",
    "    for i in range(data.shape[0]):\n",
    "        example = {}\n",
    "        #essay + essay_set + final_score + scaled_score\n",
    "        ##Strip out the parse information and the phrase labels---we don't need those here\n",
    "        text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', data['essay'][i].lower())\n",
    "        example['essay'] = text[1:] #essay\n",
    "        example['essay_set'] = data['essay_set'][i] #essay_set\n",
    "        example['final_score'] = data['final_score'][i] #final_score\n",
    "        example['scaled_score'] = data['scaled_score'][i] #scaled_score\n",
    "        \n",
    "        opt.append(example)\n",
    "    random.seed(123)\n",
    "    random.shuffle(opt)\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = load_data(\"./training_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'essay': 'he mood in exerpt is in between grateful and caring and hard because thats what @person1\\'s mood ended they way it created was by the type of house he living with them and and also the people @person1 about. also another mood is comfortable for example when he said \"i was born in newark, @location2, in a simple house explains his mood towards everything. ',\n",
       " 'essay_set': 5.0,\n",
       " 'final_score': 1.0,\n",
       " 'scaled_score': 0.25}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take pretrained glove as embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PADDING = \"<PAD>\"\n",
    "UNKNOWN = \"<UNK>\"\n",
    "max_seq_length = 7000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read in Glove\n",
    "embeddings_indices = {}\n",
    "f = open('glove.6B.50d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_indices[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    tokens = nltk.word_tokenize(string)\n",
    "    for index, token in enumerate(tokens):\n",
    "        if token == '@' and (index+1) < len(tokens):\n",
    "            tokens[index+1] = '@' + re.sub('[0-9]+.*', '', tokens[index+1])\n",
    "            tokens.pop(index)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_to_sentences(text, create_vocab_flag=False):\n",
    "\n",
    "    # tokenize a long text to a list of sentences\n",
    "    sents = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\!|\\?)\\s', text)\n",
    "    \n",
    "    processed_sents = []\n",
    "    for sent in sents:\n",
    "        if re.search(r'(?<=\\.{1}|\\!|\\?|\\,)(@?[A-Z]+[a-zA-Z]*[0-9]*)', sent):\n",
    "            s = re.split(r'(?=.{2,})(?<=\\.{1}|\\!|\\?|\\,)(@?[A-Z]+[a-zA-Z]*[0-9]*)', sent)\n",
    "            ss = \" \".join(s)\n",
    "            ssL = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\!|\\?)\\s', ss)\n",
    "\n",
    "            processed_sents.extend(ssL)\n",
    "        else:\n",
    "            processed_sents.append(sent)\n",
    "    \n",
    "\n",
    "    if create_vocab_flag:\n",
    "        sent_tokens = [tokenize(sent) for sent in processed_sents]\n",
    "        tokens = [w for sent in sent_tokens for w in sent]\n",
    "        return tokens\n",
    "\n",
    "    \n",
    "    sent_tokens = []\n",
    "    for sent in processed_sents:\n",
    "        s_tokens = [sent.split()]\n",
    "        sent_tokens.extend(s_tokens)\n",
    "    \n",
    "    return sent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_token = tokenize_to_sentences(data[1]['essay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dictionary(dataset):\n",
    "    \"\"\"\n",
    "    Input: @Essay_info_dict\n",
    "    output: word_indices, len(vocabulary)\n",
    "    \"\"\"\n",
    "    word_counter = collections.Counter()\n",
    "    for example in dataset:\n",
    "        word_counter.update(tokenize(example['essay']))\n",
    "        \n",
    "    vocabulary = [word for word in word_counter]\n",
    "    vocabulary = [PADDING, UNKNOWN] + vocabulary\n",
    "    \n",
    "    word_indices = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "    \n",
    "    return word_indices, len(vocabulary)\n",
    "\n",
    "def sentences_to_padded_index_sequences_character(word_indices, dataset):\n",
    "    \"\"\"\n",
    "    Annotate datasets with feature vectors. Adding right-sided padding\n",
    "    Input: vocabulary with integer, Essay_info_list\n",
    "    Output: Essay_info_list text as word_indices\n",
    "    \"\"\"\n",
    "    for example in dataset:\n",
    "        example['character_index'] = np.zeros(max_seq_length)\n",
    "        \n",
    "        token_sequence = tokenize(example['essay'])\n",
    "        padding = max_seq_length - len(token_sequence)\n",
    "        \n",
    "        for i in range(max_seq_length):\n",
    "            if i >= len(token_sequence):\n",
    "                index = word_indices[PADDING]\n",
    "                pass\n",
    "            else:\n",
    "                if token_sequence[i] in word_indices:\n",
    "                    index = word_indices[token_sequence[i]]\n",
    "                else:\n",
    "                    index = word_indices[UNKNOWN]\n",
    "            example['character_index'][i] = index\n",
    "\n",
    "def sentences_to_padded_index_sequences_sentence(word_indices, dataset):\n",
    "    \"\"\"\n",
    "    Annotate datasets with feature vectors. Adding right-sided padding\n",
    "    Input: vocabulary with integer, Essay_info_list\n",
    "    Output: Essay_info_list text as sentence_indices\n",
    "    \"\"\"\n",
    "    for example in dataset:\n",
    "        example['sentence_index'] = np.zeros((max_num_sentence, max_seq_length))\n",
    "        #print(example['essay'])\n",
    "        token_sequence = tokenize_to_sentences(example['essay'])\n",
    "       \n",
    "        #[['he', 'mood', 'in', 'exerpt', 'is', 'in', 'between']]\n",
    "        for j,seq in enumerate(token_sequence):\n",
    "            for i in range(max_seq_length):\n",
    "                if i >= len(seq):\n",
    "                    index = word_indices[PADDING]\n",
    "                    pass\n",
    "                else:\n",
    "                    if seq[i] in word_indices:\n",
    "                        index = word_indices[seq[i]]\n",
    "                    else:\n",
    "                        index = word_indices[UNKNOWN]\n",
    "                example['sentence_index'][j][i] = index\n",
    "        \n",
    "        #example['text_index_sequence'] = np.array(example['text_index_sequence'])\n",
    "        #example['index'] = torch.FloatTensor(example['text_index_sequence'])\n",
    "        #example['essay_set'] = torch.LongTensor([example['essay_set']])\n",
    "        \n",
    "def indSeq_to_oneHot(indSeq, word_indices): \n",
    "    \"\"\"\n",
    "    Convert index_sequence into 1-hot sequence\n",
    "    \"\"\"\n",
    "    print(indSeq.shape)\n",
    "    pos = list(map(int, indSeq))\n",
    "    n_values = len(word_indices)\n",
    "    return np.eye(n_values)[pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_indices, vocab_size = build_dictionary(data)\n",
    "sentences_to_padded_index_sequences_character(word_indices, data)\n",
    "sentences_to_padded_index_sequences_sentence(word_indices, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batchify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the iterator we'll use during training. \n",
    "# It's a generator that gives you one batch at a time.\n",
    "def data_iter(source, batch_size):\n",
    "    dataset_size = len(source)\n",
    "    start = -1 * batch_size\n",
    "    order = list(range(dataset_size))\n",
    "    random.shuffle(order)\n",
    "\n",
    "    while True:\n",
    "        start += batch_size\n",
    "        if start > dataset_size - batch_size:\n",
    "            # Start another epoch.\n",
    "            start = 0\n",
    "            random.shuffle(order)   \n",
    "        batch_indices = order[start:start + batch_size]\n",
    "        yield [source[index] for index in batch_indices]\n",
    "\n",
    "# This is the iterator we use when we're evaluating our model. \n",
    "# It gives a list of batches that you can then iterate through.\n",
    "def eval_iter(source, batch_size):\n",
    "    batches = []\n",
    "    dataset_size = len(source)\n",
    "    start = -1 * batch_size\n",
    "    order = list(range(dataset_size))\n",
    "    random.shuffle(order)\n",
    "\n",
    "    while start < dataset_size - batch_size:\n",
    "        start += batch_size\n",
    "        batch_indices = order[start:start + batch_size]\n",
    "        batch = [source[index] for index in batch_indices]\n",
    "        batches.append(batch)\n",
    "        \n",
    "    return batches\n",
    "\n",
    "# The following function gives batches of vectors and labels, \n",
    "# these are the inputs to your model and loss function\n",
    "def get_batch(batch):\n",
    "    vectors = []\n",
    "    labels = []\n",
    "    for dict in batch:\n",
    "        vectors.append(dict[\"sentence_index\"])\n",
    "        labels.append(dict['scaled_score'])\n",
    "    return vectors, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_iter = data_iter(data, batch_size)\n",
    "train_eval_iter = eval_iter(data[0:500], batch_size)\n",
    "dev_iter = eval_iter(data[0:500], batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectors, labels = get_batch(next(training_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[   1.,  546.,    1., ...,    0.,    0.,    0.],\n",
       "         [ 219.,  491.,  477., ...,    0.,    0.,    0.],\n",
       "         [ 442.,  357.,  620., ...,    0.,    0.,    0.],\n",
       "         ..., \n",
       "         [   0.,    0.,    0., ...,    0.,    0.,    0.],\n",
       "         [   0.,    0.,    0., ...,    0.,    0.,    0.],\n",
       "         [   0.,    0.,    0., ...,    0.,    0.,    0.]]),\n",
       "  array([[ 377.,  605.,  700., ...,    0.,    0.,    0.],\n",
       "         [   1.,  433.,   25., ...,    0.,    0.,    0.],\n",
       "         [ 546.,  113.,    1., ...,    0.,    0.,    0.],\n",
       "         ..., \n",
       "         [   0.,    0.,    0., ...,    0.,    0.,    0.],\n",
       "         [   0.,    0.,    0., ...,    0.,    0.,    0.],\n",
       "         [   0.,    0.,    0., ...,    0.,    0.,    0.]]),\n",
       "  array([[ 523.,  373.,  209., ...,    0.,    0.,    0.],\n",
       "         [ 682.,  605.,  123., ...,    0.,    0.,    0.],\n",
       "         [  81.,  141.,   49., ...,  461.,  682.,  605.],\n",
       "         ..., \n",
       "         [   0.,    0.,    0., ...,    0.,    0.,    0.],\n",
       "         [   0.,    0.,    0., ...,    0.,    0.,    0.],\n",
       "         [   0.,    0.,    0., ...,    0.,    0.,    0.]]),\n",
       "  array([[ 495.,    1.,    1., ...,    0.,    0.,    0.],\n",
       "         [   1.,  373.,  203., ...,    0.,    0.,    0.],\n",
       "         [ 373.,  674.,  138., ...,    0.,    0.,    0.],\n",
       "         ..., \n",
       "         [   0.,    0.,    0., ...,    0.,    0.,    0.],\n",
       "         [   0.,    0.,    0., ...,    0.,    0.,    0.],\n",
       "         [   0.,    0.,    0., ...,    0.,    0.,    0.]]),\n",
       "  array([[ 549.,  605.,  526., ...,    0.,    0.,    0.],\n",
       "         [ 454.,  638.,  357., ...,  149.,  546.,  490.],\n",
       "         [  23.,  696.,   25., ...,    0.,    0.,    0.],\n",
       "         ..., \n",
       "         [   0.,    0.,    0., ...,    0.,    0.,    0.],\n",
       "         [   0.,    0.,    0., ...,    0.,    0.,    0.],\n",
       "         [   0.,    0.,    0., ...,    0.,    0.,    0.]]),\n",
       "  array([[ 523.,  185.,  681., ...,    0.,    0.,    0.],\n",
       "         [ 442.,  437.,   38., ...,    0.,    0.,    0.],\n",
       "         [ 546.,  345.,   16., ...,    0.,    0.,    0.],\n",
       "         ..., \n",
       "         [   0.,    0.,    0., ...,    0.,    0.,    0.],\n",
       "         [   0.,    0.,    0., ...,    0.,    0.,    0.],\n",
       "         [   0.,    0.,    0., ...,    0.,    0.,    0.]]),\n",
       "  array([[ 522.,  701.,  143., ...,    0.,    0.,    0.],\n",
       "         [ 128.,    1.,  407., ...,    0.,    0.,    0.],\n",
       "         [  77.,  578.,  666., ...,    0.,    0.,    0.],\n",
       "         ..., \n",
       "         [   0.,    0.,    0., ...,    0.,    0.,    0.],\n",
       "         [   0.,    0.,    0., ...,    0.,    0.,    0.],\n",
       "         [   0.,    0.,    0., ...,    0.,    0.,    0.]]),\n",
       "  array([[ 582.,  682.,  219., ...,    0.,    0.,    0.],\n",
       "         [ 373.,  511.,   37., ...,    0.,    0.,    0.],\n",
       "         [ 420.,  373.,  177., ...,    0.,    0.,    0.],\n",
       "         ..., \n",
       "         [   0.,    0.,    0., ...,    0.,    0.,    0.],\n",
       "         [   0.,    0.,    0., ...,    0.,    0.,    0.],\n",
       "         [   0.,    0.,    0., ...,    0.,    0.,    0.]]),\n",
       "  array([[ 431.,  265.,  682., ...,    0.,    0.,    0.],\n",
       "         [ 446.,   13.,  265., ...,    0.,    0.,    0.],\n",
       "         [   0.,    0.,    0., ...,    0.,    0.,    0.],\n",
       "         ..., \n",
       "         [   0.,    0.,    0., ...,    0.,    0.,    0.],\n",
       "         [   0.,    0.,    0., ...,    0.,    0.,    0.],\n",
       "         [   0.,    0.,    0., ...,    0.,    0.,    0.]]),\n",
       "  array([[  39.,  605.,    1., ...,    0.,    0.,    0.],\n",
       "         [   1.,  171.,  442., ...,    0.,    0.,    0.],\n",
       "         [ 465.,  179.,    1., ...,    0.,    0.,    0.],\n",
       "         ..., \n",
       "         [   0.,    0.,    0., ...,    0.,    0.,    0.],\n",
       "         [   0.,    0.,    0., ...,    0.,    0.,    0.],\n",
       "         [   0.,    0.,    0., ...,    0.,    0.,    0.]])],\n",
       " [0.75,\n",
       "  0.75,\n",
       "  0.56666666666666665,\n",
       "  0.69999999999999996,\n",
       "  0.66666666666666663,\n",
       "  0.46666666666666667,\n",
       "  0.5,\n",
       "  0.625,\n",
       "  0.25,\n",
       "  0.90000000000000002])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 50)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['sentence_index'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#indSeq_to_oneHot(data[0]['sentence_index'], word_indices).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
