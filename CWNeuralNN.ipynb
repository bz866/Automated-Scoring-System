{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_excel('/Users/twff/Documents/NLP/proj/training_set_rel3.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12978, 28)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    opt = []\n",
    "    data = pd.read_excel(path)\n",
    "    for i in range(data.shape[0]): \n",
    "        example = {}\n",
    "        example['label'] = data['domain1_score'][i]\n",
    "        if example['label'] is None:\n",
    "            continue\n",
    "\n",
    "        # Strip out the parse information and the phrase labels---we don't need those here\n",
    "        text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', data['essay'][i])\n",
    "        example['essay'] = text[1:]\n",
    "        opt.append(example)\n",
    "\n",
    "    random.seed(1)\n",
    "    random.shuffle(opt)\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = load_data('/Users/twff/Documents/NLP/proj/training_set_rel3.xlsx')\n",
    "#valid_data = load_data('/Users/twff/Documents/NLP/proj/valid_set.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = X[0:10000]\n",
    "val_data = X[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'essay': 'n the story \"The Mooring Mast\" by @ORGANIZATION2, the builders of the Empire State Building faced obstacles in attempting to allow dirigibles to dock there. The builders needed more support for the building\\'s framework. They had to use over sixtey thousand dollars to buy correct modification. Lack of a suitable landing area. There are alot of obstacles for the dirigibles.The builders needed more support for the building\\'s framework. \"A thousand-foot dirigible moored at the top of the building, held by a single cable tether, would add stress to the buildings frame\" (@ORGANIZATION2, para @NUM1. They had to use over sixty thousand dollars to buy modifications for the stressed frame. \"Over sixty-thousand dollars worth of modifications had to be made to the building\\'s framework\" (@ORGANIZATION2, para @NUM1. There was a lack of suitable landing area. \"The one obstacle to their expanded use in New York City was the lack of a suitable landing area\" (@ORGANIZATION2, para 6.In the story \"The Mooring Mast\" by @ORGANIZATION2, this is how the builders of the Empire State building faced obstacles in attempting to allow dirigibles to dock there.',\n",
       " 'label': 2.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the iterator we'll use during training. \n",
    "# It's a generator that gives you one batch at a time.\n",
    "def data_iter(source, batch_size):\n",
    "    dataset_size = len(source)\n",
    "    start = -1 * batch_size\n",
    "    order = list(range(dataset_size))\n",
    "    random.shuffle(order)\n",
    "\n",
    "    while True:\n",
    "        start += batch_size\n",
    "        if start > dataset_size - batch_size:\n",
    "            # Start another epoch.\n",
    "            start = 0\n",
    "            random.shuffle(order)   \n",
    "        batch_indices = order[start:start + batch_size]\n",
    "        yield [source[index] for index in batch_indices]\n",
    "\n",
    "# This is the iterator we use when we're evaluating our model. \n",
    "# It gives a list of batches that you can then iterate through.\n",
    "def eval_iter(source, batch_size):\n",
    "    batches = []\n",
    "    dataset_size = len(source)\n",
    "    start = -1 * batch_size\n",
    "    order = list(range(dataset_size))\n",
    "    random.shuffle(order)\n",
    "\n",
    "    while start < dataset_size - batch_size:\n",
    "        start += batch_size\n",
    "        batch_indices = order[start:start + batch_size]\n",
    "        batch = [source[index] for index in batch_indices]\n",
    "        batches.append(batch)\n",
    "        \n",
    "    return batches\n",
    "\n",
    "# The following function gives batches of vectors and labels, \n",
    "# these are the inputs to your model and loss function\n",
    "def get_batch(batch):\n",
    "    vectors = []\n",
    "    labels = []\n",
    "    for dict in batch:\n",
    "        vectors.append(dict[\"text_index_sequence\"])\n",
    "        labels.append(dict[\"label\"])\n",
    "    return vectors, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take pretrained glove as embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PADDING = \"<PAD>\"\n",
    "UNKNOWN = \"<UNK>\"\n",
    "max_seq_length = 1063\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1063"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove.6B.50d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "def tokenize(string):\n",
    "    return string.split()\n",
    "\n",
    "def build_dictionary(dataset):\n",
    "    \"\"\"\n",
    "    Extract vocabulary and build dictionary.\n",
    "    \"\"\"  \n",
    "    word_counter = collections.Counter()\n",
    "    for example in dataset:\n",
    "        word_counter.update(tokenize(example['essay']))\n",
    "        \n",
    "    vocabulary = set([word for word in word_counter])\n",
    "    vocabulary = list(vocabulary)\n",
    "    vocabulary = [PADDING, UNKNOWN] + vocabulary\n",
    "        \n",
    "    word_indices = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "\n",
    "    return word_indices, len(vocabulary)\n",
    "\n",
    "def sentences_to_padded_index_sequences(word_indices, dataset):\n",
    "    \"\"\"\n",
    "    Annotate datasets with feature vectors. Adding right-sided padding. \n",
    "    \"\"\"\n",
    "    for example in dataset:\n",
    "        example['text_index_sequence'] = torch.zeros(max_seq_length)\n",
    "\n",
    "        token_sequence = tokenize(example['essay'])\n",
    "        padding = max_seq_length - len(token_sequence)\n",
    "\n",
    "        for i in range(max_seq_length):\n",
    "            if i >= len(token_sequence):\n",
    "                index = word_indices[PADDING]\n",
    "                pass\n",
    "            else:\n",
    "                if token_sequence[i] in word_indices:\n",
    "                    index = word_indices[token_sequence[i]]\n",
    "                else:\n",
    "                    index = word_indices[UNKNOWN]\n",
    "            example['text_index_sequence'][i] = index\n",
    "\n",
    "        example['index'] = example['text_index_sequence'].long().view(1,-1)\n",
    "        example['label'] = torch.FloatTensor([example['label']])\n",
    "\n",
    "\n",
    "word_to_ix, vocab_size = build_dictionary(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "sentences_to_padded_index_sequences(word_to_ix, train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = vocab_size\n",
    "EMBEDDING_DIM = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((MAX_NB_WORDS, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_to_ix.items():\n",
    "    #print('word is ' + word)\n",
    "    #print('i is %s' %i)\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    #print(embedding_vector)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67215, 50)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glove = torch.from_numpy(embedding_matrix).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67215"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C&W Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(glove.size(0), glove.size(1))\n",
    "embedding.weight = nn.Parameter(glove)\n",
    "embedding.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CWNeuralNN(nn.Module): # inheriting from nn.Module!\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, embeddings=None, freeze_embeddings=False):\n",
    "        super(CWNeuralNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        if embeddings is not None:\n",
    "            self.embed.weight = nn.Parameter(embeddings)\n",
    "        if freeze_embeddings:\n",
    "            self.embed.weight.requires_grad = False\n",
    "        \n",
    "        #self.dropout = nn.Dropout(p=0.5)\n",
    "        self.Conv1d = nn.Conv1d(embedding_dim, embedding_dim,3)\n",
    "        self.pool = nn.MaxPool1d(2,hidden_dim)\n",
    "        \n",
    "        #self.linear_1 = nn.Linear(embedding_dim, hidden_dim) \n",
    "        self.linear_2 = nn.Linear(1100, 32)\n",
    "        self.linear_3 = nn.Linear(32,1)\n",
    "        self.hardtanh = nn.Hardtanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)  \n",
    "        x = x.permute(0, 2, 1)  # swap to N, channel, word (32, 50, 104)\n",
    "        x = self.Conv1d(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        x = self.linear_2(x)\n",
    "        x = self.hardtanh(x)\n",
    "        x = self.linear_3(x)\n",
    "        return x\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 50\n",
    "hidden_dim = 50\n",
    "batch_size = 32\n",
    "num_train_steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = CWNeuralNN(vocab_size, emb_size, hidden_dim, glove, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
    "\n",
    "training_iter = data_iter(train_data, batch_size)\n",
    "train_eval_iter = eval_iter(train_data[0:500], batch_size)\n",
    "dev_iter = eval_iter(train_data[0:500], batch_size)\n",
    "\n",
    "#training_loop(model, criterion, optimizer, training_iter, dev_iter, train_eval_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def training_loop(model, loss, optimizer, training_iter, dev_iter, train_eval_iter):\n",
    "    step = 0\n",
    "    for i in range(num_train_steps):\n",
    "        model.train()\n",
    "        vectors, labels = get_batch(next(training_iter))\n",
    "        vectors = Variable(torch.stack(vectors).squeeze().long())\n",
    "        print(vectors.view(-1).max())\n",
    "        labels = Variable(torch.stack(labels).squeeze())\n",
    "        #print(labels)\n",
    "        model.zero_grad()\n",
    "        output = model(vectors)\n",
    "\n",
    "        lossy = loss(output, labels)\n",
    "        lossy.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print( \"Step %i; Loss %f; Train acc: %f; Dev acc %f\" \n",
    "                %(step, lossy.data[0], evaluate(model, train_eval_iter), evaluate(model, dev_iter)))\n",
    "\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67215"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 67197\n",
      "[torch.LongTensor of size 1]\n",
      "\n",
      "\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      "[torch.LongTensor of size 32]\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'torch.LongTensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-141-0fa3df4d6b7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_eval_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-131-0f7480a054d8>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(model, loss, optimizer, training_iter, dev_iter, train_eval_iter)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             print( \"Step %i; Loss %f; Train acc: %f; Dev acc %f\" \n\u001b[0;32m---> 19\u001b[0;31m                 %(step, lossy.data[0], evaluate(model, train_eval_iter), evaluate(model, dev_iter)))\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-140-c0b1d14baede>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, data_iter)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'torch.LongTensor'"
     ]
    }
   ],
   "source": [
    "training_loop(model, criterion, optimizer, training_iter, dev_iter, train_eval_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'essay': 'n the story \"The Mooring Mast\" by @ORGANIZATION2, the builders of the Empire State Building faced obstacles in attempting to allow dirigibles to dock there. The builders needed more support for the building\\'s framework. They had to use over sixtey thousand dollars to buy correct modification. Lack of a suitable landing area. There are alot of obstacles for the dirigibles.The builders needed more support for the building\\'s framework. \"A thousand-foot dirigible moored at the top of the building, held by a single cable tether, would add stress to the buildings frame\" (@ORGANIZATION2, para @NUM1. They had to use over sixty thousand dollars to buy modifications for the stressed frame. \"Over sixty-thousand dollars worth of modifications had to be made to the building\\'s framework\" (@ORGANIZATION2, para @NUM1. There was a lack of suitable landing area. \"The one obstacle to their expanded use in New York City was the lack of a suitable landing area\" (@ORGANIZATION2, para 6.In the story \"The Mooring Mast\" by @ORGANIZATION2, this is how the builders of the Empire State building faced obstacles in attempting to allow dirigibles to dock there.',\n",
       " 'index': \n",
       "  58667  51004  34248  ...       0      0      0\n",
       " [torch.LongTensor of size 1x1063],\n",
       " 'label': \n",
       "  2\n",
       " [torch.FloatTensor of size 1],\n",
       " 'text_index_sequence': \n",
       "  58667\n",
       "  51004\n",
       "  34248\n",
       "   â‹®   \n",
       "      0\n",
       "      0\n",
       "      0\n",
       " [torch.FloatTensor of size 1063]}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit(model, train, criterion, optimizer, batch_size=32,\n",
    "        shuffle=True, nb_epoch=1, validation_data=None, cuda=True):\n",
    "    # TODO: implement CUDA flags, optional metrics and lr scheduler\n",
    "    if validation_data:\n",
    "        print('Train on {} samples, Validate on {} samples'.format(len(train), len(validation_data)))\n",
    "    else:\n",
    "        print('Train on {} samples'.format(len(train)))\n",
    "\n",
    "    history = {}\n",
    "    t = tqdm_notebook(range(nb_epoch), total=nb_epoch)\n",
    "    for epoch in t:\n",
    "        loss, acc = _fit_epoch(model, train, criterion,\n",
    "                              optimizer, batch_size, shuffle)\n",
    "\n",
    "        history['loss'].append(loss)\n",
    "        history['acc'].append(acc)\n",
    "        if validation_data:\n",
    "            val_loss, val_acc = validate(model, validation_data, criterion, batch_size)\n",
    "            print(\"[Epoch {} - loss: {:.4f} - acc: {:.4f} - val_loss: {:.4f} - val_acc: {:.4f}]\".format(epoch+1,\n",
    "                                                                                                        loss,\n",
    "                                                                                                        acc,\n",
    "                                                                                                        val_loss,\n",
    "                                                                                                        val_acc))\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['val_acc'].append(val_acc)\n",
    "        else:\n",
    "            print(\"[loss: {:.4f} - acc: {:.4f}]\".format(loss, acc))\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _fit_epoch(model, data, criterion, optimizer, batch_size, shuffle):\n",
    "    model.train()\n",
    "    running_loss = AverageMeter()\n",
    "    running_accuracy = AverageMeter()\n",
    "    loader = DataLoader(data, batch_size, shuffle)\n",
    "    t = tqdm_notebook(loader, total=len(loader))\n",
    "    for data, target in t:\n",
    "        data, target = Variable(data.cuda()), Variable(target.cuda().squeeze())\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        accuracy = categorical_accuracy(target.data, output.data)\n",
    "        running_loss.update(loss.data[0])\n",
    "        running_accuracy.update(accuracy)\n",
    "        t.set_description(\"[ loss: {:.4f} | acc: {:.4f} ] \".format(\n",
    "            running_loss.avg, running_accuracy.avg))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return running_loss.avg, running_accuracy.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12978 samples\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'AverageMeter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-130-1b083675ccbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-126-49e6e89a33bf>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, train, criterion, optimizer, batch_size, shuffle, nb_epoch, validation_data, cuda)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         loss, acc = _fit_epoch(model, train, criterion,\n\u001b[0;32m---> 13\u001b[0;31m                               optimizer, batch_size, shuffle)\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-128-135306e9cda4>\u001b[0m in \u001b[0;36m_fit_epoch\u001b[0;34m(model, data, criterion, optimizer, batch_size, shuffle)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_fit_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAverageMeter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mrunning_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAverageMeter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AverageMeter' is not defined"
     ]
    }
   ],
   "source": [
    "history = fit(model, train_data, criterion, optimizer, validation_data=None, batch_size=32, nb_epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
