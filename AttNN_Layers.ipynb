{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention+CNN Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class _Encoder_SentAttCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size , batch_size,\n",
    "                 dropoutRate = 0.5, window_size = 5, filterNum = 100, max_SentNum = 100, max_WordNum = 100,\n",
    "                 embeddings = None, verbose = False):\n",
    "        super(_Encoder_AttCNN, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = self.hidden_size\n",
    "        self.output_size = self.output_size\n",
    "        self.batch_size = batch_size\n",
    "        self.window_size = window_size\n",
    "        self.filterNum = filterNum\n",
    "        self.max_SentNum = max_SentNum\n",
    "        self.max_WordNum = max_WordNum\n",
    "        sefl.dropoutRate = dropoutRate\n",
    "        \n",
    "        #Embedding Layer define --> threeDEmbedding\n",
    "        self.embed = nn.Embedding(self.vocab_size, self.embedding_dim, padding_idx=0)#(9) Word Representation Embedding layer\n",
    "        if embeddings is not None:\n",
    "            self.embed.weight = nn.Parameter(embeddings)\n",
    "        if freeze_embeddings:\n",
    "            self.embed.weight.requires_grad = False\n",
    "        self.dropout = nn.Dropout(p = self.dropoutRate)  #Dropout\n",
    "        \n",
    "        #Convolutional Layer define --> Word_CNN\n",
    "        self.Sent_Conv = nn.Conv1d(in_channels= self.embedding_dim, \n",
    "                                   out_channels = self.filterNum, \n",
    "                                   kernel_size=self.window_size,\n",
    "                                   stride = 1,\n",
    "                                   padding = 0, \n",
    "                                   bias = True)\n",
    "        \n",
    "        #Attention Layer define --> Word_Attn\n",
    "        self.word_attn_v = F.tanh(nn.Linear(self.filterNum, self.filterNum)) \n",
    "        self.word_attn_w = F.softmax(nn.Linear(self.filterNum, 1))\n",
    "        \n",
    "        self.init_weights()\n",
    "                                   \n",
    "    def _threeDEmbedding_U(self, x):\n",
    "        #x : (Batchsize, SentNum, seq_len, index(1))\n",
    "        #Output Size: (Batch, SenNum, seq_len, embdding_dim )\n",
    "        print(\"Original Input size: \", x.size())\n",
    "        ipt_shape = x.size()\n",
    "        bsz = x.size(0)\n",
    "        SentNum = x.size(1)\n",
    "        Seq_len = x.size(2)\n",
    "        #WordNum = x.size(3)\n",
    "        x = x.view(bsz, SentNum*Seq_len)  # (BatchSize, SentNum*seq_len , index)\n",
    "        x = self.embed(self.vocab_size, self.embedding_dim, padding_idx = 0) #(BatchSize, SentNum*seq_len, embedding_dim, )\n",
    "        x = x.view(bsz, SentNum, Seq_len, -1)  # Embedding_dim representation for each word\n",
    "        print(\"Size after Word Embedding :\", x.size()) \n",
    "        return x\n",
    "                                   \n",
    "    def _Word_CNN_U(self, x):\n",
    "        print(\"Size after Word Embedding and Word_CNN Input : \", x.size())\n",
    "        #Word CNN is only on single word inner level, just another representation for each word\n",
    "        #Output size should be same as the input size, except the feature dimmension for each word\n",
    "        #Output Size: (BatchSize, SentNum, Seq_len, filterNum)\n",
    "        #x : (Batchsize, SentNum, Seq_len, Embedding_dim)\n",
    "        \n",
    "        x = self.Sent_Conv(x)\n",
    "        print(\"Size of Word representation after Convolutional layer: \", x.size())\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def _Word_Attn_U(self, x):\n",
    "        #Word Attention layer use the sum of weighted word representations as the sentense representation\n",
    "        #Output size should be (BatchSize, SentNum, filterNum)\n",
    "        #x : (Batch, SentNum, Seq_len, filterNum)\n",
    "        ipt_shape = x.size()\n",
    "        bsz = x.size(0)\n",
    "        SentNum = x.size(1)\n",
    "        Seq_len = x.size(2)\n",
    "        f_Num = x.size(3)\n",
    "        mask = x # (BatchSize, SentNum, Seq_len, filterNum)\n",
    "        mask = mask.view(bsz, SentNum*Seq_len, -1) # (BatchSize, SentNum*seq_len , filterNum)\n",
    "        attn_weights = self.word_attn_w(self.word_attn_v(mask)) #(BatchSize, SentNum*seq_len, weightValue(1))\n",
    "        attn_weights = attn_weights.view(bsz, SentNum, Seq_len, -1)# (BatchSize, SentNum, Seq_len, weightValue(1)))\n",
    "        output = attn_weights * x #Broadcast product in the last dimension  # (BatchSize, SentNum, Seq_len, FilterNum(weighted values))\n",
    "        output = output.sum(2) #(13) #(BatchSize, SentNum, FilterNum(sum of weighted values of different z_i))\n",
    "        return output\n",
    "            \n",
    "                                   \n",
    "    def forward(self, x):\n",
    "        x_emb = _threeDEmbedding_U(x)\n",
    "        x_Conv = _Word_CNN_U(x_emb)\n",
    "        x_attn = _Word_Attn_U(x_Conv)\n",
    "        return x_attn\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 1\n",
    "        lin_layers = [self.Sent_Conv, self.word_attn_v]\n",
    "        lin_layers_nobias = [self.word_attn_w]\n",
    "        for layer in lin_layers + lin_layers_nobias:\n",
    "            layer.weight.data.uniform_(-initrange, initrange)\n",
    "            if layer in lin_layers:\n",
    "                layer.bias.data.fill_(0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention + LSTM Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Decoder_AttLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, filterNum, hidden_size, output_size, batch_size,\n",
    "                 dropoutRate = 0.5, max_SentNum = 100, max_WordNum = 100):\n",
    "        super(_Decoder_AttLSTM, self).__init__()\n",
    "        self.filterNum = filterNum\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.batch_size = batch_size\n",
    "        self.dropoutRate = dropoutRate\n",
    "        self.max_SentNum = max_SentNum\n",
    "        self.max_WordNum = max_WordNum\n",
    "        \n",
    "        #LSTM\n",
    "        self.linear_f = nn.Linear(self.filterNum + self.hidden_size, self.hidden_size)\n",
    "        self.linear_i = nn.Linear(self.filterNum + self.hidden_size, self.hidden_size)\n",
    "        self.linear_ctilde = nn.Linear(self.filterNum + self.hidden_size, self.hidden_size)\n",
    "        self.linear_o = nn.Linear(self.filterNum + self.hidden_size, self.hidden_size)\n",
    "        \n",
    "        #BiLSTM\n",
    "        \n",
    "        #Attention \n",
    "        self.attn_v = F.tanh(nn.Linear(self.hidden_size, self.hidden_size))\n",
    "        self.attn_w = F.softmax(nn.Linear(self.hidden_size, 1))\n",
    "        \n",
    "        #Dropout\n",
    "        self.dropout = nn.Dropout(p = self.dropoutRate)\n",
    "        \n",
    "        \n",
    "        #Decoder\n",
    "        self.decoder = nn.Linear(self.hidden_size, self.output_size)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    \n",
    "    def forward(self, x, hidden, c):\n",
    "        hiddens = []\n",
    "        for i in range(self.max_SentNum):\n",
    "            hidden, c = self._lstm_step(x[i].squeeze(), hidden, c)\n",
    "            hiddens.append(hidden)\n",
    "            hidden_drop = self.dropout(hidden)\n",
    "            \n",
    "        output = torch.stack(hiddens)\n",
    "        text_rep = self._Sent_Attn_U(output)\n",
    "        \n",
    "        scaled_score = F.sigmoid(self.decoder(text_rep))\n",
    "        return scaled_score\n",
    "        \n",
    "        \n",
    "        \n",
    "    def _lstm_step(self, ipt_s, hid, c_t):\n",
    "        ipt_s = self.dropout(ipt_s)\n",
    "        combined = torch.cat((hid, ipt_s), 1)\n",
    "        f = F.sigmoid(self.linear_f(combined))\n",
    "        i = F.sigmoid(self.linear_i(combined))\n",
    "        c_tilde = F.tanh(self.linear_ctilde(combined))\n",
    "        c_t = f * c_t + i * c_tilde\n",
    "        o = F.sigmoid(self.linear_o(combined))\n",
    "        hid = o * F.tanh(c_t)\n",
    "        return hid, c_t\n",
    "    \n",
    "    def _Sent_Attn_U(self, x):\n",
    "        ipt_shape = x.size()\n",
    "        bsz = x.size(0)\n",
    "        SentNum = x.size(1)\n",
    "        Seq_f = x.size(2)\n",
    "        mask = x # (BatchSize, SentNum, Seq_f)\n",
    "        attn_weights = self.attn_w(self.attn_v(mask))# (BatchSize, SentNum, 1)\n",
    "        output = attn_weights * x # (BatchSize, SentNum, Seq_f)\n",
    "        output = output.sum(1) # (BatchSize, Seq_f) #Final text representation\n",
    "        return output\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 1\n",
    "        lin_layers = [self.linear_f, self.linear_i, self.linear_ctilde, self.linear_o. self.attn_v]\n",
    "        lin_layers_nobias = [self.attn_w]\n",
    "        for layer in lin_layers + lin_layers_nobias:\n",
    "            layer.weight.data.uniform_(-initrange, initrange)\n",
    "            if layer in lin_layers:\n",
    "                layer.bias.data.fill_(0)\n",
    "    \n",
    "    def init_hidden(self,batch_size):\n",
    "        h0 = Variable(torch.zeros(batch_size, self.hidden_size))\n",
    "        c0 = Variable(torch.zeros(batch_size, self.hidden_size))\n",
    "        return h0, c0\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = []\n",
    "a = torch.rand([10,1])\n",
    "b = torch.rand([10,1])\n",
    "ls.append(a)\n",
    "ls.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "  0.2535\n",
       "  0.3492\n",
       "  0.7258\n",
       "  0.0170\n",
       "  0.7659\n",
       "  0.1883\n",
       "  0.2387\n",
       "  0.6542\n",
       "  0.6160\n",
       "  0.9839\n",
       " [torch.FloatTensor of size 10x1], \n",
       "  0.2146\n",
       "  0.7935\n",
       "  0.9036\n",
       "  0.3694\n",
       "  0.6767\n",
       "  0.4687\n",
       "  0.2924\n",
       "  0.9960\n",
       "  0.6635\n",
       "  0.0904\n",
       " [torch.FloatTensor of size 10x1]]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,.,.) = \n",
       "  0.2535\n",
       "  0.3492\n",
       "  0.7258\n",
       "  0.0170\n",
       "  0.7659\n",
       "  0.1883\n",
       "  0.2387\n",
       "  0.6542\n",
       "  0.6160\n",
       "  0.9839\n",
       "\n",
       "(1 ,.,.) = \n",
       "  0.2146\n",
       "  0.7935\n",
       "  0.9036\n",
       "  0.3694\n",
       "  0.6767\n",
       "  0.4687\n",
       "  0.2924\n",
       "  0.9960\n",
       "  0.6635\n",
       "  0.0904\n",
       "[torch.FloatTensor of size 2x10x1]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,.,.) = \n",
       "   0   2   0  ...    0   0   0\n",
       "   0   1   2  ...    1   2   2\n",
       "   1   1   1  ...    2   2   0\n",
       "     ...       ⋱       ...    \n",
       "   1   1   0  ...    0   2   2\n",
       "   0   0   0  ...    0   2   0\n",
       "   2   0   2  ...    0   2   1\n",
       "\n",
       "(1 ,.,.) = \n",
       "   1   1   2  ...    0   2   1\n",
       "   0   2   0  ...    1   0   1\n",
       "   2   1   2  ...    2   0   1\n",
       "     ...       ⋱       ...    \n",
       "   2   0   2  ...    1   0   2\n",
       "   0   0   2  ...    1   1   1\n",
       "   2   2   0  ...    1   2   0\n",
       "\n",
       "(2 ,.,.) = \n",
       "   0   0   0  ...    2   0   0\n",
       "   2   0   2  ...    0   2   0\n",
       "   2   0   1  ...    0   2   1\n",
       "     ...       ⋱       ...    \n",
       "   2   0   1  ...    2   0   1\n",
       "   1   0   1  ...    2   2   1\n",
       "   0   1   1  ...    2   2   2\n",
       "...\n",
       "\n",
       "(29,.,.) = \n",
       "   0   1   1  ...    1   0   0\n",
       "   0   0   0  ...    1   2   2\n",
       "   1   2   2  ...    0   1   0\n",
       "     ...       ⋱       ...    \n",
       "   0   2   1  ...    1   2   2\n",
       "   2   0   0  ...    1   0   1\n",
       "   2   1   1  ...    0   0   2\n",
       "\n",
       "(30,.,.) = \n",
       "   0   2   0  ...    1   2   0\n",
       "   1   0   0  ...    2   2   0\n",
       "   1   0   1  ...    0   2   2\n",
       "     ...       ⋱       ...    \n",
       "   1   2   0  ...    1   2   0\n",
       "   2   0   2  ...    0   0   1\n",
       "   0   0   2  ...    2   1   2\n",
       "\n",
       "(31,.,.) = \n",
       "   1   0   1  ...    0   0   2\n",
       "   0   2   1  ...    1   2   1\n",
       "   1   2   2  ...    1   2   2\n",
       "     ...       ⋱       ...    \n",
       "   0   2   1  ...    0   0   2\n",
       "   1   2   2  ...    1   1   1\n",
       "   0   1   0  ...    1   2   2\n",
       "[torch.LongTensor of size 32x10x10]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.floor(torch.rand(32,10,10)*3).long()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[          0,           0, 30064771075,  4564281360,           0]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy_mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.0000e+00  0.0000e+00  3.0065e+10  4.5643e+09  0.0000e+00\n",
       "[torch.FloatTensor of size 1x5]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_layer.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 10])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 10, 10, 1])\n",
      "torch.Size([32, 10, 10, 1])\n",
      "torch.Size([32, 10, 1])\n"
     ]
    }
   ],
   "source": [
    "embed_layer = nn.Embedding(1,5)\n",
    "numpy_mtx = np.ndarray(shape = (1,5), dtype = int)\n",
    "embed_layer.weight.data.copy_(torch.from_numpy(numpy_mtx))\n",
    "embed_layer.weight.requires_grad = False\n",
    "\n",
    "input_shape = x.size()\n",
    "print(x.size())\n",
    "bs = x.size(0)\n",
    "seqnum = x.size(1)\n",
    "seq_len = x.size(2)\n",
    "#word_len = x.size(3)\n",
    "#x = x.view(bs,seqnum*seq_len, -1)# (N*seq_len, word_len)\n",
    "x = x.view(bs, seqnum*seq_len)\n",
    "print(x.size())\n",
    "#x = embed_layer(x) # (N*seq_len, word_len, embd_size)\n",
    "x = x.view(bs, seqnum, seq_len, 1)\n",
    "print(x.size())\n",
    "x = x.view(bs,seqnum,seq_len, -1) # (N, seq_len, word_len, embd_size)\n",
    "print(x.size())\n",
    "x = x.sum(2) # (N, seq_len, embd_size)\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  6.46103893e+18,   8.99664134e+18,   9.06173763e+18,\n",
       "         1.15735443e+19,   9.01007606e+18])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones(400).dot(numpy_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ndarray(shape = (4000,50), dtype = int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,.,.) = \n",
       "   4\n",
       "   4\n",
       "   6\n",
       "   9\n",
       "   4\n",
       "   4\n",
       "   6\n",
       "\n",
       "(1 ,.,.) = \n",
       "   4\n",
       "   9\n",
       "   3\n",
       "   3\n",
       "   6\n",
       "   7\n",
       "   3\n",
       "\n",
       "(2 ,.,.) = \n",
       "   6\n",
       "   6\n",
       "   5\n",
       "   6\n",
       "   5\n",
       "   5\n",
       "   6\n",
       "[torch.LongTensor of size 3x7x1]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.4294  0.2548  0.3158\n",
       " 0.2820  0.2318  0.4862\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(torch.rand([2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.7976\n",
      " 0.6155\n",
      "[torch.FloatTensor of size 2x1]\n",
      "\n",
      "\n",
      " 0.7866  0.9444  0.5443\n",
      " 0.4137  0.2194  0.1141\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand([2,1])\n",
    "print(a)\n",
    "b = torch.rand([2,3])\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.6274  0.7532  0.4341\n",
      " 0.2546  0.1350  0.0703\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c = a*b\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.8820\n",
       " 0.8882\n",
       " 0.5043\n",
       "[torch.FloatTensor of size 3]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
